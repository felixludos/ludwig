
_meta.script_name: eval

seed: 1000000009

_base: [demo]
#_base: [s/dp, ttt/take-the-middle, j/client]
#_base: [s/zcot, ttt/take-the-middle, j/client]
#_base: [s/dp, ttt/take-the-middle, j/client]
#_base: [s/dpp, ttt/take-the-middle, j/client]
#_base: [s/dpp, ttt/take-the-middle, j/client]
#_base: [s/tool, ttt/tools, ttt/take-the-middle, j/client]
#_base: [s/tool, ttt/tools, ttt/take-the-middle, j/manual]
#_base: [s/tool, ttt/tools, ttt/take-the-middle, j/client, c/vllm]
#_base: [s/tool, ttt/tools, ttt/take-the-middle, j/format]
#_base: [s/tool, ttt/tools, ttt/take-the-middle, j/client, c/azure]
#_base: [s/tool, ttt/tools, ttt/take-the-middle, j/client, c/vllm]

limit: 8

use-wandb: no

root: local_data/

# addr: http://wagner.is.localnet:8000
#client.addr: 8001
#judge.addr: 8000


# resume: p6088_sfaa9_250515-120654_0b0d

#client._mod.logged: yes

max-tokens: 1024
#max-tokens: 4096 # needed for qwen3-8b due to reasoning
#max-tokens: 1024

#temperature: 0.6
#top-p: 0.95


##################

# _base: [venice-sent, c/vllm]

# #template: venice/analysis
# template: venice/analysis-yaml

#############################
#
#_base: [1step, c/vllm]
##_base: [1step, c/openai]
##judge.template: judge-binary-nothink
#
##protocol.name: "{task.domain}-{judge.format_type}-{judge.pred_type}-{'_'.join(task.clues)}"
##protocol.name: "{task.domain}_{judge.format_type}_{judge.pred_type}_{'-'.join(task.clues)}_{now.strftime('%y%m%d-%H%M%S')}"
#
#protocol.name: "debug_{task.domain}_{judge.name}_{'-'.join(task.clues)}_{now.strftime('%y%m%d-%H%M%S')}_{unique[:4]}"
#
#
##resume: food_1step-json-prob_profile-responses-behavior_250511-133413
#
##resume: food_1step-json-prob_update2_250511-211320
#
#domain: news
##clues: [profile, responses, behavior]
#clues: [update2]
#
##zero-based: yes
##indexed: yes
#
##template: "{task_context}\n\n{question}\n/nothink"
##strategy.template: "{question}"
#
##pred-type: binary

